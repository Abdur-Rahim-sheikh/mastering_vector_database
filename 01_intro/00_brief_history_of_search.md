## সার্চ এর একটি সংক্ষিপ্ত ইতিহাস

মানুষের স্মৃতি সবসময় নির্ভরযোগ্য নয়। তাই জ্ঞানকে যখন থেকেই লিখিত আকারে সংরক্ষণ করার চেষ্টা করছি, তখন থেকেই আমাদের দরকার হয়েছে এমন কোনো উপায়, যাতে প্রাসঙ্গিক তথ্য বারবার পুরো বই না পড়ে খুঁজে পাওয়া যায়। সেই ভাবনা থেকেই কিছু অসাধারণ মেধাবী মানুষ উদ্ভাবন করেন **"ইনভার্টেড ইনডেক্স"** পদ্ধতি। সহজভাবে বললে, এটা এক ধরনের সূচিপত্র যা সাধারণত বইয়ের শেষে থাকে — যেখানে গুরুত্বপূর্ণ শব্দগুলো সাজানো থাকে বর্ণানুক্রমে, আর প্রতিটি শব্দের পাশে লেখা থাকে কোন কোন পৃষ্ঠায় তা রয়েছে।

আগে এটা হতো একেবারেই হাতে তৈরি, অনেক পরিশ্রম আর সময় দিয়ে বানাতে হতো। কিন্তু ডিজিটাল যুগে এসেই কাজটা অনেক সহজ হয়ে গেছে — যদিও মূল নীতিগুলো এখনো একই আছে। তখন যেমন কাজ করত, আজও তেমনই করছে।

ধরো তুমি কোনো নির্দিষ্ট বইতে একটি নির্দিষ্ট বিষয়ের খোঁজ করছো। তুমি যদি সেই বিষয়ের সাথে সম্পর্কিত সঠিক শব্দটা জানো, তাহলে সরাসরি সংশ্লিষ্ট পৃষ্ঠায় চলে যেতে পারবে। তবে, যদি শব্দটা না জানো — তাহলে হয়তো কয়েকবার চেষ্টা করে ভুল করতে হবে, অথবা কাউকে খুঁজে বের করতে হবে যে তোমাকে ঠিকঠাক শব্দটা খুঁজে দিতে পারে।

![inverted index](/resources/inverted_index.png)

সময় কেটে গেছে, কিন্তু এই ক্ষেত্রটিতে অনেকদিন ধরেই তেমন কোনো বড় পরিবর্তন আসেনি। তবে টেক্সট ডেটার পরিমাণ দিনে দিনে প্রচণ্ড গতিতে বাড়তে শুরু করল। তাই আমরা সেই পুরনো ইনভার্টেড ইনডেক্সকে ঘিরে নানান নতুন প্রসেস তৈরি করতে শুরু করলাম।

উদাহরণস্বরূপ, আমরা ব্যবহারকারীদের একাধিক শব্দ দিয়ে সার্চ করার সুযোগ দিলাম, এবং সেই শব্দগুলোকে ছোট ছোট অংশে ভেঙে ফেলার কৌশল চালু করলাম। এতে এমন কিছু ডকুমেন্ট খুঁজে পাওয়া সম্ভব হলো যেগুলোতে হয়তো সব কিওয়ার্ড নেই, কিন্তু কিছু অংশ মিলছে। আমরা শব্দকে তাদের মূল রূপে রূপান্তর করা শুরু করলাম — যেন আরও বেশি মিল খোঁজা যায়। স্টপওয়ার্ডগুলো বাদ দেওয়া শুরু করলাম — যেমন "and", "the", "is" — যেগুলো মূল খোঁজার কাজে তেমন সাহায্য করে না।

এইভাবে ধীরে ধীরে আমাদের সার্চ সিস্টেম হয়ে উঠল আরও বেশি ইউজার-ফ্রেন্ডলি।

তবুও, এই পুরো প্রক্রিয়ার মূল ভিত্তিটা সেই পুরনো, সরল, কিওয়ার্ড-ভিত্তিক অনুসন্ধান — যেটা মধ্যযুগ থেকেই ব্যবহৃত হয়ে আসছে — শুধু এখন একটু আধুনিক রূপে, খানিকটা ঘষামাজা করে।

![Text tokenization](/resources/text_tokenization.png)

কারিগরি দৃষ্টিকোণ থেকে বললে, আমরা ডকুমেন্ট এবং কোয়েরিগুলোকে এমন একধরনের sparse vector-এ রূপান্তর করি, যেখানে প্রতিটি পজিশনের সাথে একটি করে শব্দ যুক্ত থাকে পুরো শব্দকোষ (dictionary) থেকে।

যদি ইনপুট টেক্সটে কোনো নির্দিষ্ট শব্দ থাকে, তাহলে সেই শব্দের জন্য নির্ধারিত পজিশনে ভেক্টরটি পায় একটি non-zero মান। কিন্তু বাস্তবে, একটি টেক্সটে সাধারণত শ’খানেকের বেশি ভিন্ন শব্দ থাকে না। অথচ শব্দকোষে থাকতে পারে হাজার হাজার শব্দ। ফলে ভেক্টরগুলোর বেশিরভাগ পজিশনই হয়ে যায় শূন্য (zero), আর মাত্র অল্প কিছু পজিশনে থাকে non-zero মান। তাই এগুলোকে বলা হয় sparse vector — মানে ছড়ানো ছিটানো মানযুক্ত ভেক্টর।

এই ধরনের ভেক্টর ব্যবহার করে আমরা সাধারণত শব্দ-ভিত্তিক similarity হিসাব করি — মানে, কোন কোন ডকুমেন্টে একাধিক শব্দ মিলছে, সেগুলো খুঁজে বের করা। যার সঙ্গে ইনপুটের সবচেয়ে বেশি ওভারল্যাপ থাকবে, সেটাই সবচেয়ে প্রাসঙ্গিক ধরে নেওয়া হয়।

![sparse token](sparse_token.png)

Sparse vector-গুলোর একটি গুরুত্বপূর্ণ বৈশিষ্ট্য হলো—এগুলোর dimensionality বা মাত্রা হয় অনেক বেশি, যা পুরো শব্দকোষের (dictionary) আকারের সমান। আর এই শব্দকোষ আমরা স্বয়ংক্রিয়ভাবে ইনপুট ডেটা থেকেই তৈরি করি।

এর মানে, প্রতিটি ভেক্টর আসলে একটি নির্দিষ্ট টেক্সটের প্রতিনিধিত্ব করে, যেখানে কোন কোন শব্দ ব্যবহৃত হয়েছে — তার একটা ছায়া থাকে এই ভেক্টরের ভেতরে। তাই যদি আমাদের কাছে কোনো একটি sparse vector থাকে, তাহলে আমরা আংশিকভাবে অনুমান করতে পারি ওই টেক্সটে কী কী শব্দ ছিল।

### প্রতিনিধিত্বের বিপ্লব

বর্তমানে NLP-র জন্য মেশিন লার্নিং গবেষণার সবচেয়ে আলোচিত বিষয় হলো Deep Language Model তৈরি। এই পদ্ধতিতে একটি নিউরাল নেটওয়ার্ক বিশাল পরিমাণ টেক্সট ডেটা ইনপুট হিসেবে নেয় এবং প্রতিটি শব্দকে একটি গাণিতিক ভেক্টর-এর মাধ্যমে উপস্থাপন করে।

এই ভেক্টরগুলো এমনভাবে তৈরি করা হয়, যাতে অর্থে বা প্রেক্ষিতে মিল থাকা শব্দগুলো কাছাকাছি ভেক্টরে রূপান্তরিত হয়। অর্থাৎ, দুইটি শব্দ যদি প্রায় একই অর্থ বহন করে বা প্রায় একই ধরনের বাক্যে ব্যবহৃত হয়, তাহলে তাদের ভেক্টরগুলোও প্রায় একই হবে।

এছাড়া, আমরা চাইলে প্রতিটি শব্দের ভেক্টরের গড় (average) নিয়েও পুরো একটি বাক্য, অনুচ্ছেদ বা কোয়েরির জন্য একটি সম্মিলিত ভেক্টর তৈরি করতে পারি।

এভাবে, টেক্সটকে শুধু অক্ষর বা শব্দ হিসেবে না দেখে, অর্থপূর্ণ সংখ্যাগত উপস্থাপনায় রূপান্তর করা যায় — যেটা মেশিনের কাছে বিশ্লেষণযোগ্য হয়। এটাই মূলত আধুনিক NLP-র ভিত্তি।

![Neural Representation](/resources/neural_representation.png)

আমরা নিউরাল নেটওয়ার্ক থেকে পাওয়া dense vector-গুলোকে ব্যবহার করতে পারি টেক্সট ডেটার একটি ভিন্ন ধরনের উপস্থাপন (representation) হিসেবে।

এগুলোকে dense বলা হয় কারণ এখানে প্রায় সব পজিশনেই কিছু না কিছু মান থাকে — অর্থাৎ, আগের sparse vector-এর মতো শূন্য (zero) দিয়ে ভরা থাকে না।

আরেকটি বড় পার্থক্য হলো, dense vector-গুলোর dimension তুলনামূলকভাবে কম — সাধারণত কয়েকশ বা সর্বোচ্চ কয়েক হাজার মাত্রার হয়ে থাকে।

তবে এর একটা অসুবিধাও আছে — আমরা যদি এই dense vector-গুলো দেখেও ফেলি, তবুও টেক্সটের মধ্যে কী লেখা আছে তা বোঝা সম্ভব না। কারণ এখানে আর প্রতিটি dimension কোনো নির্দিষ্ট শব্দের উপস্থিতিকে প্রকাশ করে না।

তবে এই dense vector-গুলোর শক্তি হলো — এরা শব্দ নয়, বরং অর্থ (meaning) বোঝে। তাই Large Language Model (LLM)-গুলো অটোSynonym হ্যান্ডেল করতে পারে — দুইটা ভিন্ন শব্দ হলেও যদি তাদের অর্থ কাছাকাছি হয়, তাহলে ভেক্টরও কাছাকাছি হবে।

আরও বড় ব্যাপার হলো, যদি এই মডেলগুলো বহুভাষিক (multilingual) ডেটাতে ট্রেন করা হয়ে থাকে, তাহলে একই বাক্য যদি বিভিন্ন ভাষায় লেখা হয় — যেমন “আমি ভালো আছি” এবং “I am fine” — তাহলে তারাও প্রায় একই ভেক্টর তৈরি করবে। এই ভেক্টরগুলোকে বলা হয় embeddings।

এখন যদি আমাদের কাছে একটি বিশাল ডেটাবেস থাকে যেখানে এসব embeddings রাখা আছে, তাহলে আমরা কোনো নতুন বাক্যের embedding তৈরি করে তা অন্যগুলোর সাথে distance হিসাব করে মিল খুঁজে বের করতে পারি — মানে কোন টেক্সটটা অর্থের দিক থেকে সবচেয়ে কাছাকাছি, সেটা বের করা যায়।

![Compressed data](/resources/compressed_data.png)
